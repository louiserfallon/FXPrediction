---
title: "BDF"
author: "Louise Fallon"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(readxl) #for read_excel
library(ggplot2) #for ggplot
library(reshape2) #for melt
library(knitr) #for kable
library(plyr) #for ddply
library(gridExtra) #for gridarrange
library(glmnet) #for glm
library(caret) #for train control
knitr::opts_chunk$set(echo = FALSE)
```

#What contemporaneous relationships do you think there should be between these variables and currency returns (i.e., percentage changes in the spot rate)?

* X1, interest rates, a higher inflation rate differential to the dollar should increase spot price, as investors move their money to banks where the interest rates are higher
* X2, inflation, a higher inflation differential to the dollar should decrease spot prices as investors and consumers update expectations that the currency will buy fewer local goods and services in the future
* X3, industrial production, a higher IP differential to the dollar should increase spot price, as this is a proxy for growth in the economy
* X4, money supply, a higher money supply should decrease spot price, as, if money demand is kept constant, this will decrease the value of the currency as per the rules of supply and demand (this is similar to the inflation variable)

However, the correlations between these variables are not consistent across time or across currencies, as can be seen by Figure 1.

```{r}
##creating 1 df from each sheet in the variable construction
y <- read_excel("variable_construction.xlsx", sheet = 6)
X1 <- read_excel("variable_construction.xlsx", sheet = 7)
X2 <-read_excel("variable_construction.xlsx", sheet = 10)
X3 <-read_excel("variable_construction.xlsx", sheet = 9)
X4 <-read_excel("variable_construction.xlsx", sheet = 8)

##creating 9 dataframes (one for each currency) with the relevant variables
##and putting them into a list (need a list in order to loop)
dflist <- list()
#putting ys next to lagged xs
for (i in 1:9) dflist[[i]] <- data.frame(yt=y[2:nrow(y),i+1],
                          X1=X1[1:(nrow(y)-1),i+1],
                          X2=X2[1:(nrow(y)-1),i+1],
                          X3=X3[1:(nrow(y)-1),i+1],
                          X4=X4[1:(nrow(y)-1),i+1])
```

Results of a 20-window length rolling linear regression, compared to a historical mean model, including the MSE, the R2OOS, and the average value of each of the betas for each currency:

```{r}
windowlength <- 10

#initialise vectors to store average betas for each variable
meanx1beta <- vector()
meanx2beta <- vector()
meanx3beta <- vector()
meanx4beta <- vector()
MSE.linear <- vector()
MSE.mean <- vector()
R2OOS <- vector()

for (j in 1:9) {
prd.mean <- vector()
prd.linear <- vector()
x1betas <- vector()
x2betas <- vector()
x3betas <- vector()
x4betas <- vector()
  
for (tau in windowlength:(nrow(dflist[[j]])-1)) {
##HISTORICAL MEAN MODEL
      prd.mean <- c(prd.mean,mean(y[(tau-windowlength+1):tau,j+1]))
##LINEAR MODEL
#create model on training data within the window (up to tau)
      temp.mdl.linear <- lm(yt ~ ., data=dflist[[j]][(tau-windowlength+1):tau,])
#predict the tau+1th spot rate
      prd.linear <- c(prd.linear,predict(temp.mdl.linear, newdata=dflist[[j]][tau+1,2:5]))
#store the x1 beta so we can check later if the signs are usually correct
      x1betas <- c(x1betas,temp.mdl.linear$coefficients[2])
      x2betas <- c(x2betas,temp.mdl.linear$coefficients[3])
      x3betas <- c(x3betas,temp.mdl.linear$coefficients[4])
      x4betas <- c(x4betas,temp.mdl.linear$coefficients[5])}

#for each currency
#calculate the MSE, and add to vector
MSE.mean <- c(MSE.mean,(1/length(prd.mean))*sum((prd.mean-dflist[[j]][(windowlength+1):412,"yt"])^2))
MSE.linear <- c(MSE.linear,(1/length(prd.linear))*sum((prd.linear-dflist[[j]][(windowlength+1):412,"yt"])^2))
#store the mean of the betas, and add to vector
meanx1beta <- c(meanx1beta, mean(x1betas))
meanx2beta <- c(meanx2beta, mean(x2betas))
meanx3beta <- c(meanx3beta, mean(x3betas))
meanx4beta <- c(meanx4beta, mean(x4betas))
#calculate Rsquared OOS
#(note not exactly the same as the notes but both numerator + denominator on the
#right are multiplied by 1/the number of predictions so mathematically the same
R2OOS <- c(R2OOS,1-(MSE.linear[j]/MSE.mean[j]))
}


#display data frame of the MSE and average betas
displaydf <- data.frame(Currency=c("AUD","CAD","CHF",
                                 "EUR","GBP","JPY",
                                 "NOK","NZD","SEK"),
                      meanMSE=MSE.mean,
                      linearMSE=MSE.linear,
                      R2OOS=R2OOS,
                      meanx1beta=meanx1beta,
                      meanx2beta=meanx2beta,
                      meanx3beta=meanx3beta,
                      meanx4beta=meanx4beta)
kable(displaydf)
```

#LASSO parameter tuning
The LASSO method extends the Linear Regression method from minimising $RSS$ to minimising 
$RSS+\lambda|\beta_k|)$. Using 5-fold cross validation across the full dataset, the lambda with the lowest mean squared error is identified for each currency.

:( lasso is choosing lamdbas that mean that no variables are chosen

Need to discuss if this is the correct method, or we should be doing something more about predicting just the next value.
```{r}      
x1beta.lasso <- vector()
x2beta.lasso <- vector()
x3beta.lasso <- vector()
x4beta.lasso <- vector()
lambda.lasso <- vector()
prd.fulllasso <- vector()
MSE.fulllasso <- vector()
R2OOS <- vector()

for (j in 1:9)
{ #could make folds so that the training data is the length of the window    
  cvfit <-  cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),
                               alpha = 1, standardize=TRUE, nfolds=40)
      lambda.lasso <- c(lambda.lasso,cvfit$lambda.min)
      x1beta.lasso <- c(x1beta.lasso,coef(cvfit, s = "lambda.min")[2])
      x2beta.lasso <- c(x2beta.lasso,coef(cvfit, s = "lambda.min")[3])
      x3beta.lasso <- c(x3beta.lasso,coef(cvfit, s = "lambda.min")[4])
      x4beta.lasso <- c(x4beta.lasso,coef(cvfit, s = "lambda.min")[5])
      
      #nonrollingregression
      mdl.fulllasso <- glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),
                               alpha = 1, lambda=cvfit$lambda.min,
                               standardize=TRUE)
      prd.fulllasso <- predict(mdl.fulllasso, newx = as.matrix(dflist[[j]][,2:5]), s = "lambda.min")
      MSE.fulllasso <- c(MSE.fulllasso,(1/length(prd.fulllasso))*sum((prd.fulllasso-dflist[[j]]["yt"])^2))}

#display data frame of the MSE and average betas
displaydf.lasso <- data.frame(Currency=c("AUD","CAD","CHF",
                                 "EUR","GBP","JPY",
                                 "NOK","NZD","SEK"),
                      MSE=MSE.fulllasso,
                      lambda.lasso=lambda.lasso,
                      x1beta.lasso=x1beta.lasso,
                      x2beta.lasso=x2beta.lasso,
                      x3beta.lasso=x3beta.lasso,
                      x4beta.lasso=x4beta.lasso)
kable(displaydf.lasso)
```

trying to find the best alpha
AUD

```{r}
j <- 1
foldid=sample(1:10,size=length(dflist[[j]][,1]),replace=TRUE)
cv1=cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),foldid=foldid,alpha=1)
cv.5=cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),foldid=foldid,alpha=.5)
cv0=cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),foldid=foldid,alpha=0)
##plot only looks good when you knit
par(mfrow=c(2,2))
plot(cv1);plot(cv.5);plot(cv0)
plot(log(cv1$lambda),cv1$cvm,pch=19,col="red",xlab="log(Lambda)",ylab=cv1$name)
points(log(cv.5$lambda),cv.5$cvm,pch=19,col="grey")
points(log(cv0$lambda),cv0$cvm,pch=19,col="blue")
legend("topleft",legend=c("alpha= 1","alpha= .5","alpha 0"),pch=19,col=c("red","grey","blue"))
```

CAD

```{r}
j <- 2
foldid=sample(1:10,size=length(dflist[[j]][,1]),replace=TRUE)
cv1=cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),foldid=foldid,alpha=1)
cv.5=cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),foldid=foldid,alpha=.5)
cv0=cv.glmnet(as.matrix(dflist[[j]][,2:5]),
                               as.matrix(dflist[[j]][,1]),foldid=foldid,alpha=0)
##plot only looks good when you knit
par(mfrow=c(2,2))
plot(cv1);plot(cv.5);plot(cv0)
plot(log(cv1$lambda),cv1$cvm,pch=19,col="red",xlab="log(Lambda)",ylab=cv1$name)
points(log(cv.5$lambda),cv.5$cvm,pch=19,col="grey")
points(log(cv0$lambda),cv0$cvm,pch=19,col="blue")
legend("topleft",legend=c("alpha= 1","alpha= .5","alpha 0"),pch=19,col=c("red","grey","blue"))
```

```{r eval=FALSE}
#Finding the tuning parameters for the elastic net method:
#Doesn't work
j <- 1
lambda <- 10^seq(2, -2, length = 100)
alpha <- seq(0,1,length = 10)

#set up cross validation method for train function
trn <- trainControl(method = "repeatedCV", number = 10, repeats = 5)

#set up search grid for alpha and lambda parameters
search.grid <- expand.grid(.alpha = alpha, .lambda = lambda)

#perform cross validationforecasting salary based on all features
set.seed(4243)
my.train <- train(yt ~ ., data = dflist[[j]], method = "glmnet", tuneGrid = search.grid,
                  trControl = trn, standardize = TRUE, maxit = 1000000)

#plot CV performance
plot(my.train)

#return best turning parameters
my.train$bestTune
#retrieve best model (model with the best alpha)
my.glmnet.model <- my.train$finalModel
#print coefficients of the final model (model with optimal lambda)
coef(my.glmnet.model, s = my.train$bestTune$lambda)

```

#LASSO

```{r}
windowlength <- 20

#initialise vectors to store average betas for each variable
meanx1beta <- vector()
meanx2beta <- vector()
meanx3beta <- vector()
meanx4beta <- vector()
MSE.lasso <- vector()
MSE.mean.lasso <- vector()
R2OOS <- vector()

for (j in 1:9) {
prd.mean.lasso <- vector()
prd.lasso <- vector()
x1betas <- vector()
x2betas <- vector()
x3betas <- vector()
x4betas <- vector()
  
for (tau in windowlength:(nrow(dflist[[j]])-1)) {
##HISTORICAL MEAN MODEL
      prd.mean.lasso <- c(prd.mean.lasso,mean(y[(tau-windowlength+1):tau,j+1]))
##LASSO MODEL
#create model on training data within the window (up to tau)
      temp.mdl.lasso <- glmnet(as.matrix(dflist[[j]][(tau-windowlength+1):tau,2:5]),
                               as.matrix(dflist[[j]][(tau-windowlength+1):tau,1]),
                               alpha = 1, lambda=lambda.lasso[j],
                               standardize=TRUE)
#predict the tau+1th spot rate
      prd.lasso <- c(prd.lasso,predict(temp.mdl.lasso, newx=as.matrix(dflist[[j]][tau+1,2:5])))
#store the x1 beta so we can check later if the signs are usually correct
      x1betas <- c(x1betas,coef(temp.mdl.lasso)[2])
      x2betas <- c(x2betas,coef(temp.mdl.lasso)[3])
      x3betas <- c(x3betas,coef(temp.mdl.lasso)[4])
      x4betas <- c(x4betas,coef(temp.mdl.lasso)[5])}

#for each currency
#calculate the MSE, and add to vector
MSE.mean.lasso <- c(MSE.mean.lasso,(1/length(prd.mean.lasso))*sum((prd.mean.lasso-dflist[[j]][(windowlength+1):412,"yt"])^2))
MSE.lasso <- c(MSE.lasso,(1/length(prd.lasso))*sum((prd.lasso-dflist[[j]][(windowlength+1):412,"yt"])^2))
#store the mean of the betas, and add to vector
meanx1beta <- c(meanx1beta, mean(x1betas))
meanx2beta <- c(meanx2beta, mean(x2betas))
meanx3beta <- c(meanx3beta, mean(x3betas))
meanx4beta <- c(meanx4beta, mean(x4betas))
#calculate Rsquared OOS
#(note not exactly the same as the notes but both numerator + denominator on the
#right are multiplied by 1/the number of predictions so mathematically the same
R2OOS <- c(R2OOS,1-(MSE.lasso[j]/MSE.mean[j]))
}


#display data frame of the MSE and average betas
displaydf <- data.frame(Currency=c("AUD","CAD","CHF",
                                 "EUR","GBP","JPY",
                                 "NOK","NZD","SEK"),
                      meanMSE=MSE.mean.lasso,
                      lassoMSE=MSE.lasso,
                      R2OOS=R2OOS,
                      meanx1beta=meanx1beta,
                      meanx2beta=meanx2beta,
                      meanx3beta=meanx3beta,
                      meanx4beta=meanx4beta)
kable(displaydf)
```

\newpage

(can make this prettier if we want to use it, struggling to make the red text bold)

```{r fig.height=11, fig.align="center", fig.width=10}
plotlist <- list()
for (i in 1:9)
{
#get the name of the plot as the currency
plotname <- c("AUD","CAD","CHF",
              "EUR","GBP","JPY",
              "NOK","NZD","SEK")[i]

#melt data so that each y can be matched to each of the lagged variables
meltdf <- melt(dflist[[i]], id="yt")
#create plot with points and smoothed line
p <- ggplot(meltdf , aes(y=value, x=yt)) + geom_point(alpha=0.2) + geom_smooth() + facet_grid( variable ~ ., scales="free") + theme_light()
#add correlations
cors <- ddply(meltdf, .(variable), summarise, cor = round(cor(yt, value), 2))
#put correlations 25% of the way up the x axis of each plot
cors$yaxis <- c((max(dflist[[i]][["X1"]])-min(dflist[[i]][["X1"]]))/4
                +min(dflist[[i]][["X1"]]),
               (max(dflist[[i]][["X2"]])-min(dflist[[i]][["X2"]]))/4
                +min(dflist[[i]][["X2"]]),
               (max(dflist[[i]][["X3"]])-min(dflist[[i]][["X3"]]))/4
                +min(dflist[[i]][["X3"]]),
               (max(dflist[[i]][["X4"]])-min(dflist[[i]][["X4"]]))/4
                +min(dflist[[i]][["X4"]]))
#put correlations 1/8th of the way up the x axis of the plot
cors$xaxis <-  rep((max(dflist[[i]][["yt"]])-min(dflist[[i]][["yt"]]))/8 + min(dflist[[i]][["yt"]]),4)
#add correlations to the plot
p <- p + geom_text(data=cors, aes(label=cor, x=xaxis, y=yaxis, color="red")) + theme(legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + ylab("") + ggtitle(plotname)
##add plot to the list
plotlist[[plotname]] <- p
}
grid.arrange(plotlist[["AUD"]],plotlist[["CAD"]],plotlist[["CHF"]],
             plotlist[["EUR"]],plotlist[["GBP"]],plotlist[["JPY"]],
             plotlist[["NOK"]],plotlist[["NZD"]],plotlist[["SEK"]], nrow=3)
```

Figure 1: Correlation plot of lagged X variables on Y variable for all currencies, blue line showing the smoothed trend, and red text showing the correlation.